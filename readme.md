# ğŸ‰ RAG: Game of Thrones Chatbot

## ğŸ“‘ Table of Contents

- [ğŸ‰ RAG: Game of Thrones Chatbot](#-rag-game-of-thrones-chatbot)
  - [ğŸ“‘ Table of Contents](#-table-of-contents)
  - [ğŸ§  Introduction](#-introduction)
  - [ğŸ“š Dataset](#-dataset)
  - [ğŸ“ Project Structure](#-project-structure)
  - [ğŸ§© How It Works: RAG Pipeline](#-how-it-works-rag-pipeline)
    - [1. ğŸ”§ Data Preparation](#1--data-preparation)
    - [2. ğŸ” Query + Retrieval](#2--query--retrieval)
    - [3. ğŸ’¬ Generation](#3--generation)
  - [ğŸ§  Models \& Tooling](#-models--tooling)
  - [ğŸ§ª Evaluation with RAGAS](#-evaluation-with-ragas)
  - [âœ¨ Example Queries](#-example-queries)
  - [ğŸš€ Launch Instructions](#-launch-instructions)
  - [ğŸ§± Docker Setup ](#ğŸ§±-docker-setup)
    - [ğŸ“‹ Prerequisites](#-prerequisites)
    - [ğŸ“¦ Build Docker Image](#-build-docker-image)

![Python](https://img.shields.io/badge/Python-3.11-blue)
![LangChain](https://img.shields.io/badge/LangChain-Integrated-yellow)
![Docker](https://img.shields.io/badge/Docker-Ready-green)
![License](https://img.shields.io/badge/License-MIT-lightgrey)
![RAGAS](https://img.shields.io/badge/Evaluation-RAGAS-critical)

![A Song of Ice and Fire Box Set](images/george-r-r-martin-s-a-game-of-thrones-5-book-boxed-set-song-of-ice-and-fire-series.jpg)

---

## ğŸ§  Introduction

Welcome to my **Game of Thrones AI Chatbot**, powered by **RAG (Retrieval-Augmented Generation)**.  
This project combines my passion for the _Game of Thrones_ universe with what Iâ€™ve learned during a recent AI Apprenticeship.

The goal?  
To build a **lore-accurate question-answering system** that understands the complex world of Westeros â€” from the Red Keep to the Wall.

---

## ğŸ“š Dataset

The chatbotâ€™s knowledge base consists of the following five books by George R. R. Martin:

- _A Game of Thrones_
- _A Clash of Kings_
- _A Storm of Swords_
- _A Feast for Crows_
- _A Dance with Dragons_

> âš ï¸ **Note:** Due to copyright restrictions, I cannot provide the raw text files or links to the books.

---

## ğŸ“ Project Structure

<summary><strong>ğŸ“ Project Directory Tree with Explanations</strong></summary>

```plaintext
Game of Thrones Chatbot/
â”œâ”€â”€ conf/                  # Config files for the entire pipeline
â”‚   â”œâ”€â”€ embeddings/        # Embedding-specific settings
â”‚   â”œâ”€â”€ preprocessing/     # Preprocessing (cleaning, deduplication) configs
â”‚   â”œâ”€â”€ text_splitter/     # Chunking and sliding window logic configs
â”‚   â”œâ”€â”€ evaluate.yaml      # Config for evaluation using RAGAS
â”‚   â”œâ”€â”€ inference.yaml     # Inference parameters (LLM, retriever settings)
â”‚   â”œâ”€â”€ logging.yaml       # Logging setup
â”‚   â””â”€â”€ training.yaml      # Any training-specific parameters
â”‚
â”œâ”€â”€ data/                  # Data storage folders
â”‚   â”œâ”€â”€ answers/           # Ground truth or LLM-generated answers
â”‚   â”œâ”€â”€ datasets/          # Game of Thrones Books
â”‚   â”œâ”€â”€ embeddings/        # Vector Database
â”‚   â””â”€â”€ inference/         # Prompt templates
â”‚
â”œâ”€â”€ docker/                # Dockerfile and Docker-specific configs
â”‚
â”œâ”€â”€ images/                # Diagrams and logos used in the README
â”‚
â”œâ”€â”€ scripts/               # Shell scripts to automate tasks
â”‚   â”œâ”€â”€ build_docker.sh    # Builds the Docker image
â”‚   â””â”€â”€ run_docker.sh      # Runs the Docker container
â”‚
â””â”€â”€ src/                   # Core Python source code
    â”œâ”€â”€ embeddings/        # Code to generate and manage embeddings
    â”œâ”€â”€ evaluation/        # RAGAS-based evaluation pipeline
    â”œâ”€â”€ frontend/          # Gradio interface components
    â”œâ”€â”€ inference/         # Retrieval + Generation logic
    â”œâ”€â”€ utils/             # Helper functions, logging, formatting
    â”œâ”€â”€ .env               # API keys and environment variables
    â”œâ”€â”€ app.py             # Main entry point (Gradio app)
    â”œâ”€â”€ evaluate.py        # Entry point to run evaluation
    â”œâ”€â”€ infer.py           # Script to run inference from command line
    â””â”€â”€ train.py           # Script to run preprocessing and embedding
```

---

## ğŸ§© How It Works: RAG Pipeline

RAG stands for **Retrieval-Augmented Generation** â€” a framework that enhances LLMs by pairing them with external knowledge sources.

![RAG](images/classicalrag.png)

Hereâ€™s how the pipeline operates behind the scenes:

### 1. ğŸ”§ Data Preparation

- The full book texts are **chunked** into overlapping sections.
- These chunks are converted into **dense embeddings** using a pre-trained model (e.g., OpenAI, HuggingFace).
- We store the chunks + metadata in a **FAISS** vector database for efficient similarity search.

### 2. ğŸ” Query + Retrieval

- When a user submits a question (e.g., _"What happened at the Red Wedding?"_), itâ€™s also embedded.
- The embedding is used to search the vector store and retrieve the top `k` most relevant passages.

### 3. ğŸ’¬ Generation

- The **query + retrieved context** is injected into a prompt template.
- This template is passed to a **Large Language Model (LLM)** like GPT-4, Gemini, or Mixtral via LangChain.
- The model then generates an answer grounded in the books' content â€” not just general internet knowledge.

---

## ğŸ§  Models & Tooling

| Component           | Tool/Library |
| ------------------- | ------------ |
| Embeddings          | Hugging Face |
| Vector Store        | FAISS        |
| LLM Interface       | LangChain    |
| Reranker (Optional) | Cohere       |
| Observability       | Langfuse     |
| Evaluation Metrics  | RAGAS        |
| Frontend            | Gradio       |

> ğŸ” Supports both single-query and multi-query retrieval  
> ğŸ”„ Reranking enabled via `ContextualCompressionRetriever`

---

## ğŸ§ª Evaluation with RAGAS

I also implemented an **automated evaluation pipeline** using [RAGAS](https://github.com/explodinggradients/ragas):

- âœ… **Faithfulness** â€“ Is the answer backed by the retrieved context?
- âœ… **Answer Relevance** â€“ Does the answer fully respond to the query?
- âœ… **Context Precision & Recall** â€“ Are the retrieved documents relevant and sufficient?

Evaluations are reproducible and logged to **Langfuse**, enabling robust testing across LLMs, retrievers, and prompts.

---

## âœ¨ Example Queries

- _Who is Jon Snow's real mother?_
- _What are the three betrayals Daenerys was warned of?_
- _Describe the Red Wedding in detail._
- _What does the prophecy of Azor Ahai say?_

The chatbot provides **text-grounded answers**, referencing exact content from the books â€” not hallucinations.

---

## ğŸš€ Launch Instructions

To launch the Gradio interface locally:

```bash
python app.py
```

---

## ğŸ§± Docker Setup

To containerize and run the chatbot using Docker, follow these steps:

### ğŸ“‹ Prerequisites

Before building the Docker image, ensure the following:

- Docker is installed on your system ([Docker Desktop](https://docs.docker.com/get-docker/))
- Your `.env` file (containing API keys and environment variables) exists under `src/.env`
- Able to run **bash script**:

  - **Linux/macOS:** Bash is usually pre-installed; you can run scripts directly in the terminal.
  - **Windows:**
    - Use **Git Bash** (install [Git for Windows](https://git-scm.com/download/win)) to run bash scripts.
    - Alternatively, use **Windows Subsystem for Linux (WSL)** if installed.

### ğŸ“¦ Build Docker Image

Run this command from the project root to build the image:

```bash
bash scripts/build_docker.sh
```

After building the image, run the below to spin up a docker container:

```bash
bash scripts/run_docker.sh
```
