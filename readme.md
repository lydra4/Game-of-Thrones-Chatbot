# üêâ RAG: Game of Thrones Chatbot

## üìë Table of Contents

- [üêâ RAG: Game of Thrones Chatbot](#-rag-game-of-thrones-chatbot)
  - [üìë Table of Contents](#-table-of-contents)
  - [üß† Introduction](#-introduction)
  - [üìö Dataset](#-dataset)
  - [üìÅ Project Structure](#-project-structure)
  - [üß© How It Works: RAG Pipeline](#-how-it-works-rag-pipeline)
    - [1. üîß Data Preparation](#1--data-preparation)
    - [2. üîé Query + Retrieval](#2--query--retrieval)
    - [3. üí¨ Generation](#3--generation)
  - [üß† Models \& Tooling](#-models--tooling)
  - [üß™ Evaluation with RAGAS](#-evaluation-with-ragas)
  - [‚ú® Example Queries](#-example-queries)
  - [üöÄ Launch Instructions](#-launch-instructions)
  - [üß± Docker Setup ](#-docker-setup)
    - [üìã Prerequisites](#-prerequisites)
    - [üì¶ Build Docker Image](#-build-docker-image)

---

![Python](https://img.shields.io/badge/Python-3.11-blue)
![LangChain](https://img.shields.io/badge/LangChain-Integrated-yellow)
![Docker](https://img.shields.io/badge/Docker-Ready-green)
![License](https://img.shields.io/badge/License-MIT-lightgrey)
![RAGAS](https://img.shields.io/badge/Evaluation-RAGAS-critical)
![GitHub stars](https://img.shields.io/github/stars/lydra4/Deep-Learning?style=social)
![GitHub issues](https://img.shields.io/github/issues/lydra4/Deep-Learning)
![GitHub last commit](https://img.shields.io/github/last-commit/lydra4/Deep-Learning)

![Box set of George R.R. Martin's books - A Song of Ice and Fire](images/george-r-r-martin-s-a-game-of-thrones-5-book-boxed-set-song-of-ice-and-fire-series.jpg)

---

## üß† Introduction

Welcome to my **Game of Thrones AI Chatbot**, powered by **RAG (Retrieval-Augmented Generation)**.  
This project combines my passion for the _Game of Thrones_ universe with what I‚Äôve learned during a recent AI Apprenticeship.

The goal?  
To build a **lore-accurate question-answering system** that understands the complex world of Westeros ‚Äî from the Red Keep to the Wall.

---

## üìö Dataset

The chatbot‚Äôs knowledge base consists of the following five books by George R. R. Martin:

- _A Game of Thrones_
- _A Clash of Kings_
- _A Storm of Swords_
- _A Feast for Crows_
- _A Dance with Dragons_

> ‚ö†Ô∏è **Note:** Due to copyright restrictions, I cannot provide the raw text files or links to the books.

---

## üìÅ Project Structure

<strong>üìÅ Project Directory Tree with Explanations</strong>

```plaintext
Game of Thrones Chatbot/
‚îú‚îÄ‚îÄ conf/                  # Config files for the entire pipeline
‚îÇ   ‚îú‚îÄ‚îÄ embeddings/        # Embedding-specific settings
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing/     # Preprocessing (cleaning, deduplication) configs
‚îÇ   ‚îú‚îÄ‚îÄ text_splitter/     # Chunking and sliding window logic configs
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.yaml      # Config for evaluation using RAGAS
‚îÇ   ‚îú‚îÄ‚îÄ inference.yaml     # Inference parameters (LLM, retriever settings)
‚îÇ   ‚îú‚îÄ‚îÄ logging.yaml       # Logging setup
‚îÇ   ‚îî‚îÄ‚îÄ training.yaml      # Any training-specific parameters
‚îÇ
‚îú‚îÄ‚îÄ data/                  # Data storage folders
‚îÇ   ‚îú‚îÄ‚îÄ answers/           # Ground truth or LLM-generated answers
‚îÇ   ‚îú‚îÄ‚îÄ datasets/          # Game of Thrones Books
‚îÇ   ‚îú‚îÄ‚îÄ embeddings/        # Vector Database
‚îÇ   ‚îî‚îÄ‚îÄ inference/         # Prompt templates
‚îÇ
‚îú‚îÄ‚îÄ docker/                # Dockerfile and Docker-specific configs
‚îÇ
‚îú‚îÄ‚îÄ images/                # Diagrams and logos used in the README
‚îÇ
‚îú‚îÄ‚îÄ scripts/               # Shell scripts to automate tasks
‚îÇ   ‚îú‚îÄ‚îÄ build_docker.sh    # Builds the Docker image
‚îÇ   ‚îî‚îÄ‚îÄ run_docker.sh      # Runs the Docker container
‚îÇ
‚îî‚îÄ‚îÄ src/                   # Core Python source code
    ‚îú‚îÄ‚îÄ embeddings/        # Code to generate and manage embeddings
    ‚îú‚îÄ‚îÄ evaluation/        # RAGAS-based evaluation pipeline
    ‚îú‚îÄ‚îÄ frontend/          # Gradio interface components
    ‚îú‚îÄ‚îÄ inference/         # Retrieval + Generation logic
    ‚îú‚îÄ‚îÄ utils/             # Helper functions, logging, formatting
    ‚îú‚îÄ‚îÄ .env               # API keys and environment variables
    ‚îú‚îÄ‚îÄ app.py             # Main entry point (Gradio app)
    ‚îú‚îÄ‚îÄ evaluate.py        # Entry point to run evaluation
    ‚îú‚îÄ‚îÄ infer.py           # Script to run inference from command line
    ‚îî‚îÄ‚îÄ train.py           # Script to run preprocessing and embedding
```

---

## üß© How It Works: RAG Pipeline

RAG stands for **Retrieval-Augmented Generation** ‚Äî a framework that enhances LLMs by pairing them with external knowledge sources.

![RAG](images/classicalrag.png)

Here‚Äôs how the pipeline operates behind the scenes:

### 1. üîß Data Preparation

- The full book texts are **chunked** into overlapping sections.
- These chunks are converted into **dense embeddings** using a pre-trained model (e.g., OpenAI, HuggingFace).
- We store the chunks + metadata in a **FAISS** vector database for efficient similarity search.

### 2. üîé Query + Retrieval

- When a user submits a question (e.g., _"What happened at the Red Wedding?"_), it‚Äôs also embedded.
- The embedding is used to search the vector store and retrieve the top `k` most relevant passages.

### 3. üí¨ Generation

- The **query + retrieved context** is injected into a prompt template.
- This template is passed to a **Large Language Model (LLM)** like GPT-4, Gemini, or Mixtral via LangChain.
- The model then generates an answer grounded in the books' content ‚Äî not just general internet knowledge.

---

## üß† Models & Tooling

| Component     | Tool/Library                            | Required    |
| ------------- | --------------------------------------- | ----------- |
| Embeddings    | ![Hugging Face](images/huggingface.png) | ‚úÖ          |
| Vector Store  | ![FAISS](images/faiss.jpg)              | ‚úÖ          |
| LLM Interface | ![LangChain](images/langchain.png)      | ‚úÖ          |
| Reranker      | ![Cohere](images/cohere.png)            | üîÅ Optional |
| Observability | ![Langfuse](images/langfuse.png)        | üîÅ Optional |
| Evaluation    | ![RAGAS](images/ragas.png)              | ‚úÖ          |
| Frontend      | ![Gradio](images/gradio.jpg)            | ‚úÖ          |

> üîÅ Supports both single-query and multi-query retrieval  
> üîÑ Reranking enabled via `ContextualCompressionRetriever`

---

## üß™ Evaluation with RAGAS

I also implemented an **automated evaluation pipeline** using [RAGAS](https://github.com/explodinggradients/ragas):

- ‚úÖ **Faithfulness** ‚Äì Is the answer backed by the retrieved context?
- ‚úÖ **Answer Relevance** ‚Äì Does the answer fully respond to the query?
- ‚úÖ **Context Precision & Recall** ‚Äì Are the retrieved documents relevant and sufficient?

Evaluations are reproducible and logged to **Langfuse**, enabling robust testing across LLMs, retrievers, and prompts.

---

## ‚ú® Example Queries

- _Who is Jon Snow's real mother?_
- _What are the three betrayals Daenerys was warned of?_
- _Describe the Red Wedding in detail._
- _What does the prophecy of Azor Ahai say?_

The chatbot provides **text-grounded answers**, referencing exact content from the books ‚Äî not hallucinations.

---

## Live Demo / Screenshots

## ![Live Demo](images/livedemo.gif)

## üöÄ Launch Instructions

To launch the Gradio interface locally:

```bash
python app.py
```

---

## üöÄ Setup Instructions (Non-Docker)

> Recommended for local development and debugging without Docker.

### 1. Clone the Repository

```bash
git clone https://github.com/YOUR_USERNAME/game-of-thrones-chatbot.git
cd game-of-thrones-chatbot
```

### 2. Create and Activate a Conda Environment

```bash
conda env create -f .\gotchatbot-conda-env.yaml -y
conda activate got-chatbot
```

### 3. Install Dependencies

```bash
pip install -r .\requirements.txt -r .\dev-requirements.txt
```

### 4. Add Environment Variables

Create a `.env` file inside the `src/` directory and include your API keys:

```env
OPENAI_API_KEY=your_openai_api_key
GEMINI_API_KEY=your_gemini_api_key
```

### 5. Run the Application

```bash
python src/app.py
```

This will launch the Gradio interface at:  
[Link](http://127.0.0.1:7860)

## üß± Docker Setup

To containerize and run the chatbot using Docker, follow these steps:

### üìã Prerequisites

Before building the Docker image, ensure the following:

- Docker is installed on your system ([Docker Desktop](https://docs.docker.com/get-docker/))
- Your `.env` file (containing API keys and environment variables) exists under `src/.env`
- Able to run **bash script**:

  - **Linux/macOS:** Bash is usually pre-installed; you can run scripts directly in the terminal.
  - **Windows:**
    - Use **Git Bash** (install [Git for Windows](https://git-scm.com/download/win)) to run bash scripts.
    - Alternatively, use **Windows Subsystem for Linux (WSL)** if installed.

### üì¶ Build Docker Image

Run this command from the project root to build the image:

```bash
bash scripts/build_docker.sh
```

After building the image, run the below to spin up a docker container:

```bash
bash scripts/run_docker.sh
```
