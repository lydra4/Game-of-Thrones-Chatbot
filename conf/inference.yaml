embeddings:
  # Directory path to store or load precomputed embeddings
  embeddings_path: "./data/embeddings/semantic_chunker/all-MiniLM-L6-v2"

  # Name of the FAISS index to use for similarity search
  index_name: faiss_index_got

  # Whether to display progress during embedding generation
  show_progress: True

# Path to the prompt template used during QA generation
path_to_qa_prompt: "./data/inference/qa_prompt.txt"

retrieve:
  # Number of top documents to retrieve during similarity search
  k: 5

  # Retrieval method to use: "similarity", "mmr", etc.
  search_type: "similarity"

  # Optional reranker model path or identifier (empty if not used)
  reranker_model: ""

  # Whether to use multi-query expansion to improve retrieval diversity
  use_multiquery: False

  multiquery:
    # Include the original query along with generated variations
    include_original: True

    # Path to the prompt template used to generate multiqueries
    path_to_multiquery_prompt: "./data/inference/multiquery_prompt.txt"

llm:
  # LLM model to be used for answering questions
  model: "gpt-4o-mini"

  # Temperature setting to control randomness in model output
  temperature: 0.0

  # Path to the input questions file
  path_to_qns: "./data/inference/questions.txt"

  # Path to save generated answers
  path_to_ans: "./data/answers/answers.txt"

retrieval:
  # Type of LangChain retrieval chain to use (e.g., "stuff", "map_reduce", etc.)
  chain_type: "stuff"

  # Whether to return the source documents used for generating the answer
  return_source_documents: True

  # Whether to print detailed logs during the retrieval process
  verbose: False

  # Maximum number of tokens the retrieval chain should process
  max_tokens: 15_000

gradio:
  # Path to UI captions used in the Gradio frontend
  captions_path: "data/inference/ui_captions.txt"
