embeddings:
  embeddings_path: "./data/embeddings/semantic_chunker/all-MiniLM-L6-v2" # Directory path to store or load precomputed embeddings
  index_name: faiss_index_got # Name of the FAISS index to use for similarity search
  show_progress: True # Whether to display progress during embedding generation

path_to_qa_prompt: "./data/inference/qa_prompt.txt" # Path to the prompt template used during QA generation

retrieve:
  k: 5 # Number of top documents to retrieve during similarity search
  search_type: "similarity" # Retrieval method to use: "similarity", "mmr", etc.
  reranker_model: "" # Optional reranker model path or identifier (empty if not used)
  use_multiquery: False # Whether to use multi-query expansion to improve retrieval diversity

  multiquery:
    include_original: True # Include the original query along with generated variations
    path_to_multiquery_prompt: "./data/inference/multiquery_prompt.txt" # Path to the prompt template used to generate multiqueries

llm:
  model: "gpt-4o-mini" # LLM model to be used for answering questions
  temperature: 0.0 # Temperature setting to control randomness in model output
  path_to_qns: "./data/inference/questions.txt" # Path to the input questions file
  path_to_ans: "./data/answers/answers.txt" # Path to save generated answers

retrieval:
  chain_type: "stuff" # Type of LangChain retrieval chain to use (e.g., "stuff", "map_reduce", etc.)
  return_source_documents: True # Whether to return the source documents used for generating the answer
  verbose: False # Whether to print detailed logs during the retrieval process
  max_tokens: 15_000 # Maximum number of tokens the retrieval chain should process

gradio:
  captions_path: "data/inference/ui_captions.txt" # Path to UI captions used in the Gradio frontend
